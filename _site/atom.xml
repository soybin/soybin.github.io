<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>pablo peñarroja millán</title>
 <link href="https://soybin.github.io/atom.xml" rel="self"/>
 <link href="https://soybin.github.io/"/>
 <updated>2020-07-13T16:16:34+02:00</updated>
 <id>https://soybin.github.io/</id>
 <author>
   <name>pablo peñarroja millán</name>
   <email>soybinary@gmail.com</email>
 </author>

 
 <entry>
   <title>using computer vision to virtualize motion</title>
	 <link href="https://soybin.github.io/articles/2020/06/20/using-computer-vision-to-virtualize-motion.html"/>
   <updated>2020-06-20T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/articles/2020/06/20/using-computer-vision-to-virtualize-motion</id>
   <content type="html">&lt;h1 id=&quot;using-computer-vision-to-virtualize-motion&quot;&gt;using computer vision to virtualize motion&lt;/h1&gt;

&lt;h2 id=&quot;preamble&quot;&gt;preamble&lt;/h2&gt;
&lt;p&gt;Today, I’ll reflect the logic behind vaaac; a single header file library which uses OpenCV to compute the angles at which a user’s arm is inclined, and to trigger an event right after a very specific motion has been performed by the user’s hand.&lt;br /&gt;
I first wrote the library, and then I wrote &lt;a href=&quot;https://github.com/soybin/cvgo&quot;&gt;&lt;strong&gt;&lt;em&gt;cv:go&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;; an application which overwrites memory values for the player’s view angles in the videogame CS:GO with the values provided by vaaac for the user’s arm inclination. It also allows the user to shoot whenever the motion is performed by their hand, as I demonstrate in the following video:&lt;/p&gt;
&lt;iframe width=&quot;560px&quot; height=&quot;315px&quot; src=&quot;https://www.youtube.com/embed/YiGEf9hP55E?start=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;understanding-the-problem&quot;&gt;understanding the problem&lt;/h2&gt;
&lt;p&gt;The problem would be something along these lines:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‘Make a computer understand the real world, given a two-dimensional pixel matrix.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;That problem does certainly look very intimidating. Perhaps because we, as humamns, have more data about our surroundings, such as depth, or perhaps because we don’t fully understand how we, as humans, understand the world around us in the first place.&lt;/p&gt;

&lt;p&gt;Making the problem a bit more specific will make it easier to approach:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‘Make a computer identify a person, compute at what angle their arms are inclined, and check if the user has performed a trigger motion with their hands, by quickly moving their index finger up and back down.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you lack basic image processing knowledge, looking at this problem may seem intimidating. But it becomes easier as you learn about the existence of some methods commonly used in this field.&lt;/p&gt;

&lt;p&gt;The following is how I personally approached the problem, but keep in mind there’s virtually an infinite number of ways of approaching this very same problem.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-users-skin-tone&quot;&gt;getting the user’s skin tone&lt;/h2&gt;
&lt;p&gt;Finding the user’s skin tone is crucial to carrying out the next steps of the process. That’s because we’ll want to differentiate between the user and everything else in the image.&lt;/p&gt;

&lt;p&gt;One smart solution to finding the user’s skin tone can be to use a Haar Cascade in order to find the face of the user, take a sample of the forehead, and apply a threshold. The reson I didn’t end up using this option for vaaac is because Haar cascade files tend to be pretty big, and I wanted to write a library as lightweight as possible.&lt;/p&gt;

&lt;p&gt;What I did instead is quite simple and intuitive, although it requires a bit of work from the user. I asked the user to fill a portion of the screen with a sample of their arm’s skin, and to press any button whenever the requirement was met. The program then takes a copy of the pixel matrix within the screen portion area bounds and computes the average color. As seen in the following gif:&lt;br /&gt;
&lt;img src=&quot;../../../../gifs/vaaac01.gif&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;binarization-of-the-image&quot;&gt;binarization of the image&lt;/h2&gt;
&lt;p&gt;After finding the user’s color skin, the next step is to binarize the image. That means making every pixel of the image either black (if it’s not the user’s skin) or white (if it’s the user’s skin).&lt;br /&gt;
First of all we’ll convert the RGB image to HSV, that way we can ignore shadows.&lt;br /&gt;
Then we’ll check for every pixel in the image whether that pixel is equal to the average color of the user’s skin or not. However, this won’t work because of how lighting affects the color of the user’s skin. Instead, we have to check for every pixel in the image whether that pixel can be found within a range of values. This can be easily done with the OpenCV function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv::inRange(cv::Mat, cv::Scalar(skinTone - lowThreshold), cv::Scalar(skintTone + highThreshold), cv::Mat)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;isolate-a-component-with-breadth-first-search&quot;&gt;isolate a component with breadth first search&lt;/h2&gt;
&lt;p&gt;Breadth first search (bfs from now on) is a basic graph traversing algorithm (take a look &lt;a href=&quot;https://en.wikipedia.org/wiki/Breadth-first_search&quot;&gt;&lt;strong&gt;&lt;em&gt;here&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; in case you don’t know much about it).&lt;br /&gt;
A matrix such as the recently computed binary image can be easily interpreted as a graph. Now we can make an assumption, or rather a requirement: the elbow of the user must be fixed nearby the center of the image. This is completely natural, as the user would adopt this position anyways to get more screen space to aim.&lt;br /&gt;
By creating this restriction, we can have a small area in the center of the image that serves as the trigger to identify the bounds of the user’s arm.
This means that we can check every pixel of that small area for every frame, and apply bfs only if skin has been found.&lt;br /&gt;
By applying bfs we can get the bounds of the area occupied by the arm, as well as the furthest point from the center of the screen, in other words, the position of the tip of the index finger of the user’s arm.&lt;/p&gt;

&lt;p&gt;It’s worth noting that applying bfs for every single pixel would be extremely inefficient, so I added a variable &lt;em&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;/em&gt; that represents the square root of the area in pixels of each new subdivision of the image.&lt;br /&gt;
That just means that I don’t apply bfs to every individual pixel, rather I compute the average value for every subdivision of the screen with area \(2^n\).&lt;/p&gt;

&lt;p&gt;The bfs algorithm works the following way:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For every subdivision inside the area at the center of the image, check the average color of the subdivision.
    &lt;ul&gt;
      &lt;li&gt;If a subdivision has non-zero value, stop checking every subdivision inside the area at the center of the image, and start bfs by adding this subdivision to the bfs queue for processing.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;While the bfs queue is not empty
    &lt;ul&gt;
      &lt;li&gt;Update \((x, y)\) coordinate values to the current subdivision position coordinates (relative to the center of the screen). Since bfs will check the farthest point in the last place, no check is necessary.&lt;/li&gt;
      &lt;li&gt;Check every neighbor subdivision of the currently processed subdivision, and enqueue the neighbors with non-zero value.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a simplification, but the actual implementation in vaaac is not that different.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (; !q.empty(); ) {
	std::pair&amp;lt;int, int&amp;gt; xy = q.front();
	int x = xy.first, y = xy.second;
	q.pop();
	if (x &amp;lt; 0 || y &amp;lt; 0 || x + BFS_SAMPLE_SIZE &amp;gt; res || y + BFS_SAMPLE_SIZE &amp;gt; res || visited[x][y]) continue;
	visited[x][y] = true;
	if (cv::mean(mask(cv::Rect(x, y, BFS_SAMPLE_SIZE, BFS_SAMPLE_SIZE)))[0] == 0) continue;
	/*
	 * update aiming point coordinates.
	 * works because the bfs algorithm always
	 * visits the farthermost element in the
	 * last place
	 */
	xAim = x;
	yAim = y;
	// update found object area bounds
	xMin = std::min(xMin, x);
	yMin = std::min(yMin, y);
	xMax = std::max(xMax, x);
	yMax = std::max(yMax, y);
	// add neighbors to queue
	for (auto&amp;amp; offset : bfsOffsets) {
		q.push(std::make_pair&amp;lt;int, int&amp;gt;(x + offset.first, y + offset.second));
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that’s left to do is compute the aiming angles in degrees relative to the image bounds, and smooth them by an arbitrary factor.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// make angles
yAngle = -(double)(halfRes - yAim) / halfRes * 90.0;
xAngle = -(double)(halfRes - xAim) / halfRes * 90.0;
yAngleSmooth += (yAngle - yAngleSmooth) / (double)(AIM_SMOOTHNESS);
xAngleSmooth += (xAngle - xAngleSmooth) / (double)(AIM_SMOOTHNESS);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;event-trigger&quot;&gt;event trigger&lt;/h2&gt;
&lt;p&gt;I wanted to have some way for the user to communicate an action to the library, because just computing the arm inclination angles is probably not enough for most usage cases.&lt;br /&gt;
The motion of moving your index finger up and back down would be pretty convenient, since the main idea I had in mind for this library was to integrate it with an fps videogame, so that’s what I did.&lt;/p&gt;

&lt;p&gt;This problem is fairly easy, but it needed some more work than I initially expected.&lt;br /&gt;
The data structure I used to solve it is a double ended queue, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::deque&amp;lt;T&amp;gt;&lt;/code&gt; in c++ stl. This is because we’ll want to remove elements from the back and add to the front, but still be able to access the elements in between.&lt;br /&gt;
Anyways, it boils down to the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the deque is empty. If that’s the case, add the current index finger coordinates to it and continue.&lt;/li&gt;
  &lt;li&gt;Else if the bool &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; is true. If that’s the case, check if the y-coordinate of the index finger is greater than that of the last deque element. Otherwise, check if the distance between the last enqueued y-coordinate and the first enqueued y-coordinate is greater or equal than a certain threshold (this is to avoid noise from being detected as an event). If that’s the case, set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; to false. Otherwise, clear the deque.&lt;/li&gt;
  &lt;li&gt;Else if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; is set false, check if the y-coordinate of the index finger is smaller than that of the last deque element. Otherwise, check if the distance between the last enqueued y-coordinate and the first enqueued y-coordinate is greater or equal than a certain threshold (this is to avoid noise from being detected as an event). If that’s the case, set the boolean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;triggered&lt;/code&gt; to true meaning that, in this frame, an event was triggered by the user. Then clear the deque and set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; to true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is the code used in vaaac to implement the previous concept. Remember that you can take a full look to the source code at &lt;a href=&quot;https://github.com/soybin/vaaac&quot;&gt;&lt;em&gt;&lt;strong&gt;https://github.com/soybin/vaaac&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if (!yxDeltaSize) {
	yxDelta.push_back({ yAim, xAim });
	yxDeltaSize = 1;
} else if (increment) {
	if (yAim &amp;lt;= yxDelta[yxDeltaSize - 1].first) {
		++yxDeltaSize;
		yxDelta.push_back({ yAim, xAim });
	} else {
		int yDelta = yxDelta[0].first - yxDelta[yxDeltaSize - 1].first;
		if (yDelta &amp;gt;= TRIGGER_MINIMUM_DISTANCE_PIXELS &amp;amp;&amp;amp; yDelta &amp;lt;= TRIGGER_MAXIMUM_DISTANCE_PIXELS) {
			increment = false;
		} else {
			yxDeltaSize = 0;
			yxDelta.clear();
		}
	}
} else {
	if (yAim &amp;gt;= yxDelta[yxDeltaSize - 1].first) {
		yxDelta.push_back({ yAim, xAim });
		++yxDeltaSize;
	} else {
		std::pair&amp;lt;int, int&amp;gt; left = yxDelta[0];
		std::pair&amp;lt;int, int&amp;gt; right = yxDelta[yxDeltaSize - 1];
		bool ok = true;
		ok &amp;amp;= std::abs(left.first - right.first) &amp;lt;= TRIGGER_ALLOWED_Y_DEVIATION_PIXELS;
		ok &amp;amp;= std::abs(left.second - right.second) &amp;lt;= TRIGGER_ALLOWED_X_DEVIATION_PIXELS;
		if (ok) {
			triggered = true;
			yAim = left.first;
			xAim = left.second;
		}
		increment = true;
		yxDelta.clear();
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;This was a very fun project to work on. I had never done anything related to image processing, but the learning curve was mostly linear thanks to OpenCV’s design and comprehensive documentation.&lt;br /&gt;
Computer vision has an exciting long way to go. The possibilities are very exciting and, in the literal sense of the word, endless.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>raymarch the geometry & pathtrace the scene</title>
	 <link href="https://soybin.github.io/articles/2020/05/22/raymarch-the-geometry-and-pathtrace-the-scene.html"/>
   <updated>2020-05-22T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/articles/2020/05/22/raymarch-the-geometry-and-pathtrace-the-scene</id>
   <content type="html">&lt;!---[*fractal rendered with idyll using raymarching and path tracing*](/images/idyll/first.png){:width=&quot;100%&quot;}---&gt;
&lt;h1 id=&quot;raymarch-the-geometry--pathtrace-the-scene&quot;&gt;raymarch the geometry &amp;amp; pathtrace the scene&lt;/h1&gt;

&lt;h2 id=&quot;preamble&quot;&gt;preamble&lt;/h2&gt;
&lt;p&gt;Today, I’ll superficially explain and reflect on raymarching and pathtracing; the two main algorithms behind the rendering pipeline that I came up with when developing  &lt;a href=&quot;https://www.youtube.com/watch?v=cFykbtJmg4A&quot;&gt;&lt;strong&gt;&lt;em&gt;idyll&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;, a fractal engine.&lt;/p&gt;

&lt;h2 id=&quot;definitions&quot;&gt;definitions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ray&lt;/em&gt;: simple structure composed of two \(n\) dimensional vectors: an &lt;strong&gt;origin&lt;/strong&gt; vector, which indicates at which point in space the ray originates, and a unit &lt;strong&gt;direction&lt;/strong&gt; vector, which denotes the direction in which the ray will move.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Geometry&lt;/em&gt;: any independent model, shape or object present in the scene.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Scene&lt;/em&gt;: \(n\) dimensional space in which the geometry is represented.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;raymarching&quot;&gt;raymarching&lt;/h2&gt;
&lt;p&gt;Raymarching is basically a raycasting-based algorithm used to interpret a scene.&lt;/p&gt;

&lt;p&gt;The algorithm itself doesn’t compute lighting, shadowing nor reflections on its own, it rather gives you information about whether a ray emitted from a pixel does or doesn’t intersect the surface of any geometry in the scene. In other words, it calculates the depth of the scene.&lt;/p&gt;

&lt;h3 id=&quot;distance-estimators&quot;&gt;distance estimators&lt;/h3&gt;
&lt;p&gt;Imagine that you’re working on a 3-dimensional space, &lt;em&gt;in order to perform raymarching, you need a function that tells you how far away is any given point in space from the closest point to the surface of the geometry&lt;/em&gt;. That’s called a &lt;strong&gt;&lt;em&gt;distance estimator&lt;/em&gt;&lt;/strong&gt;, DE from now on.&lt;/p&gt;

&lt;p&gt;The most basic DE functions represent primitive objects, and &lt;a href=&quot;https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm&quot;&gt;&lt;strong&gt;&lt;em&gt;this&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; is a fantastic resource by Íñigo Quílez on them.&lt;/p&gt;

&lt;p&gt;For example, here you can see the DE function for a sphere \(sphereDE\), where \(P\) is the point in space that you’re calculating how far apart from the sphere it is, and \(S\) is the scale of the sphere (assuming the sphere is located at the origin of coordinates):&lt;/p&gt;

&lt;p&gt;\[ sphereDE(P, S) = length(P) - S\]&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;double sphereDE(vector3 P, double S) {
	return length(P) - S;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;what-to-do-with-a-de&quot;&gt;what to do with a DE&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Once you know how far away from something you are, you know you can safely move that distance in any direction, without having to worry about hitting that object.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let a camera be placed anywhere in your scene with a distance of 10 meters from the origin of coordinates (0, 0, 0), and looking towards it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let a ray have its origin (ori) at the position of the camera in your scene, and give it a direction (dir) towards the center of coordinates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let a variable total distance (tDist) represent how far away has the ray moved from its origin, and it’ll be initialized to zero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, calculate the DE from the point O + D * D’ (the origin of the ray plus the direction of the ray times the distance the ray moved).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then you can safely &lt;em&gt;&lt;strong&gt;march&lt;/strong&gt;&lt;/em&gt; the ray by the distance returned by the DE function, which means you can add DE(O + D * D’) to D.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat this process until DE(ori + dir * tDist) returns a value smaller than a certain arbitrary threshold (1e-4 for example), which means that the ray is close enough to the surface of the geometry for it to be considered an intersection. Obviously the ray won’t get to hit the geometry itself because of Zeno’s paradox, so it’s an estimation, hence the name distance estimator.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This process should be computed for each pixel on the screen, and it’ll provide an accurate interpretation of the model, no matter how complex the distance estimator may be.&lt;/p&gt;

&lt;p&gt;The pseudocode for the simplest version of this algorithm would look something like the following:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// distance threshold
const double MIN_DISTANCE = 1e-4

/*
 * the maximum number of steps allowed. 
 * if this amount of steps, 'raymarch'
 * will return 0.0
 */
const int MAX_STEP = 64
 
color raymarch(vector3 ori, vector3 dir) {
	double tDist = 0.0
	int i
	for (i = 0; i &amp;lt; MAX_STEP; ++i) {
		double curDist = sphereDE(ori + dir * tDist)
		// in case the threshold is reached
		if (curDist &amp;lt; MIN_DISTANCE) {
			break
		}
		// else, add the marched distance
		// to the distance variable
		tDist += curDist
	}
	return color(1.0 - (double)i / (double)MAX_STEP)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a very simple implementation, and if you were to use the returned value from this function as the value for the three color channels of each pixel, you would get a black and white image with some very obvious banding artifacts (since we’re deriving the return value from two integers).&lt;/p&gt;

&lt;p&gt;There’s also a lot of things I’ve left out of this explanation because they’re beyond the scope of this article, such as primitive shapes transformation and combination (somewhat similar to intersection boolean operations), spatial transforms, and even &lt;a href=&quot;https://www.youtube.com/watch?v=zy1l1SLJDo4&quot;&gt;infinite repetition&lt;/a&gt;, among many others. Minimally modifying the DE for your geometry will dramatically alter the end result of the geometry.&lt;/p&gt;

&lt;h3 id=&quot;ray-direction&quot;&gt;ray direction&lt;/h3&gt;
&lt;p&gt;I’ve skipped over this until now, because it’s not vital to conceptually understanding raymarching. However, it is a very important implementation detail in most cases. The following, is an example of a formula for calculating the direction that each pixel’s ray \(rayDirection(P_x, P_y)\) should follow, given a desired field of view \(fov\) in radians:&lt;/p&gt;

&lt;p&gt;Let the constant \(P_x\) be equal to the pixel x-coordinate on your screen.&lt;br /&gt;
Let the constant \(P_y\) be equal to the pixel y-coordinate on your screen.&lt;br /&gt;
Let the constant \(width\) be equal to your screen’s width in pixels.&lt;br /&gt;
Let the constant \(height\) be equal to your screen’s height in pixels.&lt;/p&gt;

&lt;p&gt;\[rayDirection(P_x, P_y) = [x, y, -z]\]
\[x = P_x + 0.5 - \frac{width}{2}\]
\[y = P_y + 0.5 - \frac{height}{2}\]
\[z = \frac{height}{tan(\frac{fov}{2})}\]&lt;/p&gt;

&lt;p&gt;This formula applies to most physically based rendering (pbr) techniques, and the implementation itself is very straighforward.&lt;/p&gt;

&lt;h3 id=&quot;real-world-usage&quot;&gt;real world usage&lt;/h3&gt;
&lt;p&gt;As long as you can derive a function that, for any given point in 3d space, returns its distance from the closest point to any geometry in the scene, you can render pretty much anything with a great performance. Adding dynamic lighting, shadowing, occlusion and surface color would have a very low computational cost. Even changing the geometry’s scale, position and rotation. Anything is valid, because everything is dynamic and computed in real time.&lt;/p&gt;

&lt;p&gt;Raymarching is not mainstream in the videogame industry, the reason for this is that you &lt;strong&gt;&lt;em&gt;need&lt;/em&gt;&lt;/strong&gt; a DE function, and in order for it to run in real time, all of it should to be computed in a fragment shader on the GPU side. When you have a world with hundreds of independent entities interacting with each other, materials to be computed, and very specific and complex geometrical shapes, it becomes counterproductive to have an efficient DE.&lt;/p&gt;

&lt;p&gt;On the other hand, whenever you have an accurate DE, you can efficiently compute your scene using raymarching. This makes raymarching the ideal method to render complex mathematical models, such as fractals or combinations of these.&lt;/p&gt;

&lt;h2 id=&quot;pathtracing&quot;&gt;pathtracing&lt;/h2&gt;
&lt;p&gt;Pathtracing is a rendering technique derived from raytracing. We’ll discuss the differences between this two terms later in this article.&lt;/p&gt;

&lt;p&gt;There are two important constants to define when implementing a pathtracing algorithm: the samples per pixel, and the bounces per ray.&lt;/p&gt;

&lt;p&gt;The idea behind pathtracing is to &lt;em&gt;&lt;strong&gt;iteratively integrate most of the light arriving to a point in the surface of the geometry&lt;/strong&gt;&lt;/em&gt;, so that the image looks as real as possible.
To visualize it, this is the pseudocode for a very basic pathtracing algorithm (only Lambertian reflection):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// number of samples per pixel
const int SAMPLES = 256

// maximum number of allowed ray bounces
const double BOUNCES = 8;

// probability of ray reflection
const double PROB = 1/PI

// pathtracing recursively
color pathtrace (ray r, int bounces) {
	// in the ray missed
	if (!r.hit) return color(0.0)

	// in case we reached max number of bounces
	if (bounces == BOUNCES) return color(0.0)

	// get ray normal
	vector3 rayNormal = calculateNormal(r.origin, r.direction)

	// get current new ray origin
	vector3 newOrigin = r.origin + r.length * r.direction;

	// get random unit vector in the hemisphere
	// of the normal, so that the ray can bounce off
	vector3 newDir = getHemisphereRandom(rayNormal)

	// get color at the intersected point
	color color = getWorldColor(r.origin)

	// bidirectional reflectance distribution function
	double ct = dot(newDir, rayNormal)
	color brdf = r.hitReflectance / PI

	ray newRay = {newOrigin, newDir}

	color nextBounce = pathtrace(newRay, bounces + 1)

	// this is where the integration of
	// the lighting along the different
	// intersection points of the ray
	// happens
	return color + (brdf * nextBounce * ct / PROB)
}

// iterate through every pixel in the screen
for (y in height) {
	for (x in width) {
		// get ray's direction and origin
		vector3 direction = rayDirection(x, y)
		vector3 origin = {0, 0, 10.0}
		// variable storing color
		color totalColor = {0.0, 0.0, 0.0}
		// iterate for every sample and add to color
		for (sample in SAMPLES) {
			totalColor += pathtrace(origin, direction, 0)
		}
	}
	color /= samples
	pixels[y][x] = color
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What’s happening here is the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Iterate through every pixel in the screen or image.&lt;/li&gt;
  &lt;li&gt;~ Compute the ray’s origin and direction from the pixel coordinates.&lt;/li&gt;
  &lt;li&gt;~ Iterate through every sample.&lt;/li&gt;
  &lt;li&gt;~ ~ Add the return value of the function ‘pathtrace’, which randomly computes a possible value for that ray, to the ‘totalColor’ variable.&lt;/li&gt;
  &lt;li&gt;~ Divide the ‘totalColor’ variable by the number of samples to get the average value for all of the samples.&lt;/li&gt;
  &lt;li&gt;~ Set the pixel with ‘x’ and ‘y’ coordinates to the ‘totalColor’ variable value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With a high enough value for the samples and bounces variable, you can get very photorealistic images using path tracing.&lt;/p&gt;
&lt;h2 id=&quot;disambiguation&quot;&gt;disambiguation&lt;/h2&gt;

&lt;p&gt;If you’re reading this, chances are you’ve heard of raytracing, and it’s no coincidence raytracing, raymarching and pathtracing sound familiar.&lt;/p&gt;

&lt;p&gt;All three of these PBR (physically based rendering) algorithms have something in common: &lt;em&gt;&lt;strong&gt;they all use raycasting to understand a scene’s geometry&lt;/strong&gt;&lt;/em&gt;, but what each one of this algorithms accomplishes, and the way they operate is different from one another.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;You can think of raytracing as a complete method that calculates both the point of intersection and the behaviour of the ray after the intersection. Whereas raymarching only computes the point of intersection, and pathtracing only computes the behaviour of the ray after the intersection.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Even though you might think that raytracing is the union of raymarching and pathtracing, that’s not the case, because the way they carry out both calculating the point intersection and the behaviour of the ray afterwards, greatly differs from one another.&lt;/p&gt;

&lt;h3 id=&quot;raymarching-and-raytracing&quot;&gt;raymarching and raytracing&lt;/h3&gt;
&lt;p&gt;The most notorious difference between raymarching and raytracing is the following:&lt;br /&gt;
In raymarching, the intersection with the scene’s geomtry is computed using a distance estimator, as we’ve recently seen. While as in raytracing, the intersection is analitically computed, and you don’t require a DE. This makes raytracing more suitable for games, although it’s still a very expensive technique.&lt;/p&gt;

&lt;h3 id=&quot;pathtracing-and-raytracing&quot;&gt;pathtracing and raytracing&lt;/h3&gt;
&lt;p&gt;The goal of pathtracing is achieving a similar result to the original raytracing technique but with less computation.
Pathtracing and raytracing both are physically based rendering (PBR) techniques, both are expensive, and both represent reality with very high fidelity.&lt;br /&gt;
Both techniques are fundamentally similar, but their biggest difference is how a ray bounce is determined.
You could argue which one looks better, but at the end of the day, that’s subjective.&lt;/p&gt;

&lt;h2 id=&quot;union-of-raymarching-and-pathtracing&quot;&gt;union of raymarching and pathtracing&lt;/h2&gt;
&lt;p&gt;Now you can see how raymarching and pathtracing can be complementary to each other, making for a solid rendering pipeline:&lt;br /&gt;
With raymarching, you can check whether a ray intersects the geometry or not, and where. Then, you can use pathtracing to integrate the global illumination at the point of intersection between the surface of the geometry and the marched ray.&lt;/p&gt;

&lt;p&gt;Pathtracing computes the lighting, but doesn’t compute the point of intersection with the geometry. On the other hand, raymarching computes the point of intersection with the geometry, but doesn’t compute lighting. This makes the union of these two techniques ideal to render DE functions.&lt;/p&gt;
&lt;h2 id=&quot;results&quot;&gt;results&lt;/h2&gt;
&lt;p&gt;After properly implementing these techniques, and throwing some iterative fractals into the mix, I developed &lt;a href=&quot;https://github.com/soybin/idyll&quot;&gt;&lt;strong&gt;&lt;em&gt;idyll&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;, a fractal engine that can create beautiful renders like the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/idyll/first.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/images/idyll/second.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/images/idyll/third.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 
 
 <entry>
   <title>on open source</title>
	 <link href="https://soybin.github.io/thoughts/2020/06/15/on-open-source.html"/>
   <updated>2020-06-15T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/thoughts/2020/06/15/on-open-source</id>
   <content type="html">&lt;p&gt;it is very cool indeed&lt;/p&gt;

</content>
 </entry>
 
</feed>
