<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>pablo peñarroja millán</title>
 <link href="https://soybin.github.io/atom.xml" rel="self"/>
 <link href="https://soybin.github.io/"/>
 <updated>2021-03-07T09:40:09+01:00</updated>
 <id>https://soybin.github.io/</id>
 <author>
   <name>pablo peñarroja millán</name>
   <email>soybinary@gmail.com</email>
 </author>

 
 <entry>
   <title>compute clouds from randomness</title>
	 <link href="https://soybin.github.io/articles/2020/12/16/compute-clouds-from-randomness.html"/>
   <updated>2020-12-16T00:00:00+01:00</updated>
	 <id>https://soybin.github.io/articles/2020/12/16/compute-clouds-from-randomness</id>
   <content type="html">&lt;h1 id=&quot;compute-clouds-from-randomness&quot;&gt;compute clouds from randomness&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;../../../../images/ao_main_image.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;em&gt;rendered with &lt;a href=&quot;https://github.com/soybin/ao&quot;&gt;ao「青」&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;preamble&quot;&gt;preamble&lt;/h2&gt;
&lt;p&gt;Can we say there’s anything &lt;em&gt;not&lt;/em&gt; random?&lt;/p&gt;

&lt;p&gt;When looking at the picture of life from a human perspective, everything is perfectly defined; it all looks like it has a purpose. However, questioning life from a broader perspective makes the very concept of existence seem absurd. How can anything &lt;em&gt;be&lt;/em&gt;? Rather, &lt;em&gt;why&lt;/em&gt; can anything &lt;em&gt;be&lt;/em&gt;? This basic question, to which we have no satisfactory answer, is a good reminder of just how random life is. We don’t even know why we’re here, yet we know why we engage in complex activities such as finishing this sentence. We fail to understand the fundamental reasons behind our mere existence, understanding only that which we arbitrarily build on top of it.&lt;/p&gt;

&lt;p&gt;Randomness underlies the very core of existence; therefore, being able to compute it is central to this universe’s accurate simulation. That being said, let’s talk about how can we use randomness to model the specific part of the universe concerning us today.&lt;/p&gt;

&lt;h2 id=&quot;clouds&quot;&gt;clouds&lt;/h2&gt;

&lt;p&gt;Round, fluffy, and — in my opinion — quite magnificient. One could probably identify an infinite number of different clouds as clouds; there’s a common pattern to all of them. That is, they’re made of water (mostly), so the way light interacts with them is similar among the different types of clouds — giving them their characteristic looks. The main variable that makes every cloud unique is its shape. “But it doesn’t make sense to define the shape of something with randomness!” — I can hear the reader say. And the reader is partially right; it doesn’t make much sense to define the shape of something we already know with pure randomness. However, applying some rules and constraints to randomness — that is, &lt;em&gt;arranging&lt;/em&gt; randomness — or simply adding randomness to a region of a system, often yields the most organic results.&lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;noise&lt;/h2&gt;
&lt;p&gt;We may think of noise as &lt;em&gt;arranged randomness&lt;/em&gt;. Extending this line of reasoning, nature itself is arranged randomness as well. Clouds are a part of nature, which means there must be a way to arrange randomness which will inevitably result in clouds.&lt;/p&gt;

&lt;p&gt;Worley noise is a simple way to arrange randomness capable of generating Voronoi diagrams: a pattern found all over nature. My implementation is not too different from the one described in the &lt;a href=&quot;http://weber.itn.liu.se/~stegu/TNM084-2017/worley-originalpaper.pdf&quot;&gt;original paper by Steve Worley&lt;/a&gt;; thus, I won’t dwell on a detailed explanation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Imagine we have a cube.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We divide that cube into n^3 cells.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each cell, we compute three random floating point values in the range [0, 1] representing the coordinates of a point within that cell.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We assign a resolution &lt;em&gt;r&lt;/em&gt;. We loop &lt;em&gt;r^3&lt;/em&gt; times (each row, column, and depth layer), computing at every point the distance to the closest point (which turns out to be a smooth transition). To accomplish this, we can obtain the cell to which the current point belongs by computing the floor of the coordinates of the point and iterating through every adjacent cell (26 times in three dimensions).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To each texel we assign the value one minus the square root of the minimum distance to a point.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we must consider whether the current cell is located at the boundaries of the cube. If that’s the case, in order to avoid discontinuity on the resulting texture, we must take into account the cells that would be posed in the cell’s boundaries if the texture repeated continuously along every axis.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The reader can appreciate the result of the described algorithm in the following gif, where I iterate over the different depth layers of a 256x256x256 Worley noise 3D texture.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../gifs/ao_worley_simple.gif&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although this starts to look promising, the reader hasn’t probably seen many clouds with perfectly delimited boundaries such as the ones we can see here. Fortunately, that has an easy solution.&lt;/p&gt;

&lt;p&gt;Combining different layers of worley noise with different frequencies gives a much more organic result. I used a persistency parameter &lt;em&gt;p&lt;/em&gt; to determine how mixed up the layers should be. The first one has a weight of 1, the second one has a weight of &lt;em&gt;p&lt;/em&gt;, and the third one has a weight of &lt;em&gt;p^2&lt;/em&gt;. This value is normalized over &lt;em&gt;1 + p + p^2&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Three layers of worley noise with increasingly lower frequencies mixed using a persistency parameter.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../gifs/ao_worley_mixed.gif&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This looks more natural and, although we’re still a bit far away from mastering cloudcraftship, we are done with the main building block for this whole thing.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;&lt;em&gt;implementation&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;The noise textures only require to be computed once for every kind of cloud, and the algorithm to generate them is fast when working with two dimensional, low resolution textures. However, when computing three dimensional and high resolution textures (for example, the main noise texture used in ao is 256x256x256 texels by default), rendering times increase dramatically, slowing down the startup of the program. That’s why I used OpenGL compute shaders to render the noise textures, making full use of the, usually more advanced, graphics processing unit. The idea is to first compute an array of randomly generated points in the CPU side and then dispatch the compute shaders, to which I pass this array of points as SSBOs (shader storage buffer objects). The compute shaders parallelly write the resulting computations for each texel to a 3D texture (image3D).&lt;/p&gt;

&lt;h2 id=&quot;good-we-have-noise-now-what&quot;&gt;good. we have noise. now what?&lt;/h2&gt;

&lt;p&gt;Well, we need &lt;em&gt;&lt;strong&gt;even more&lt;/strong&gt;&lt;/em&gt; noise.&lt;/p&gt;

&lt;p&gt;I use three different noise textures in ao. I’ll proceed to explain each one of them in order of decreasing relevance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, a main noise 3D texture; this texture defines the main body of the clouds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second, a detail noise 3D texture; this texture should have less resolution than the main texture, and it will serve the purpose of adding detail to the edges of the cloud.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Third, a weather noise 2D (or 3D) texture; this one determines where should we consider computing clouds. Although it’s not essential for a small number of clouds, it makes it harder to notice a repetition pattern when one is looking at a large number of them, making the simulation more realistic.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Note that the number of subdivisions and persistence parameter should be modulated according to the kind of cloud intented to achieve. The same applies to the rest of variables here. Also, this is just the way I decided to do things; depending on one’s specific application, one may need -2, 2, or 20 more noise layers.)&lt;/p&gt;

&lt;p&gt;Using these noise layers, we can already build the density function \(density(\vec{pos})\), where \(\vec{pos}\) is a vector representing a point in space. There’s not a magic formula to writing a good density function, as it will depend on the project’s intrinsecalities. The only requirement for it is to return the density produced by the combination of the different noise layers properly scaled to the specified frame of reference.&lt;/p&gt;

&lt;p&gt;That just means that the function will sample values from the noise textures and combine them in a way that makes sense in our context.&lt;/p&gt;

&lt;p&gt;This is the density function I came up with for ao:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float density(vec3 pos) {
	float time = frame / 1000.0;
	vec3 lower_bound = cloud_location - cloud_volume;
	vec3 upper_bound = cloud_location + cloud_volume;

	// edge weight.
	// to not cut off the clouds abruptly
	float distance_edge_x = min(cloud_volume_edge_fade_distance, min(pos.x - lower_bound.x, upper_bound.x - pos.x));
	float distance_edge_z = min(cloud_volume_edge_fade_distance, min(pos.z - lower_bound.z, upper_bound.z - pos.z));
	float edge_weight = min(distance_edge_x, distance_edge_z) / cloud_volume_edge_fade_distance;

	// !!! -&amp;gt; round cloud based on height - probably not an efficient approach
	// https://www.desmos.com/calculator/lg2fhwtxvo
	float height = 1.0 - pow((pos.y - lower_bound.y) / (2.0 * cloud_volume.y), 4);

	// 2d worley noise to decide where can clouds be rendered
	vec2 weather_sample_location = pos.xz / noise_weather_scale + noise_weather_offset + wind_vector.xz * wind_weather_weight * time;
	float weather = max(texture(noise_weather_texture, weather_sample_location).r, 0.0);
	weather = max(weather - cloud_density_threshold, 0.0);

	// main cloud shape noise
	vec3 main_sample_location = pos / noise_main_scale + noise_main_offset + wind_vector * wind_main_weight * time;
	float main_noise_fbm = texture(noise_main_texture, main_sample_location).r;

	// total density at current point obtained from these values
	float density = max(0.0, main_noise_fbm * height * weather * edge_weight - cloud_density_threshold);

	if (density &amp;gt; 0.0) {
		// add detail to cloud's shape
		vec3 detail_sample_location = pos / noise_detail_scale + noise_detail_offset + wind_vector * wind_detail_weight * time;
		float detail_noise_fbm = texture(noise_detail_texture, detail_sample_location).r;
		density -= detail_noise_fbm * noise_detail_weight;
		return max(0.0, density * cloud_density_multiplier);
	}
	return 0.0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function samples the weather 2D noise and main 3D noise and checks whether the density is above a minimum threshold or not. In affirmative case, it subtracts the detail 3D noise from the resulting density (so that the edges look organic, as I previously mentioned). Otherwise, it returns a density of 0 (simply meaning there’s no cloud at that point).&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The distance to the edges of the rendering volume (in this case a cube, as we’ll see later on) is calculated and stored in the variable “edge_weight”. This makes it less likely for clouds to form in the edges of the rendering volume, avoiding hard discontinuities in these regions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The y-component of the \(\vec{pos}\) vector is taken into account to calculate the variable “height”, using a fourth degree polynomial to round the upper edges of the clouds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The different noise layers are scaled with the external variables (uniforms) “noise_main_scale”, “noise_weather_scale”, and “noise_detail_scale”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting density value is multiplied by a “cloud_density_multiplier” variable, which turns out to be exactly what its name is: a factor that scales the cloud’s density.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is very nice; we can now sample the density of different noise layers that resemble clouds. However, one probably wants to restrict density sampling to a specific region in space, rendering the clouds exclusively within that space. That brings our focus to raytracing.&lt;/p&gt;

&lt;h2 id=&quot;raytracing&quot;&gt;raytracing&lt;/h2&gt;

&lt;p&gt;Assuming clouds are bound to form in accurately delimited regions of space, and assuming these regions can be expressed as simple geometrical shapes (cube, cylinder, etc.), raytracing is an efficient approach.&lt;/p&gt;

&lt;p&gt;By “raytracing” I simply mean a function that tells us, if we moved along a certain direction, how much would we have to move in order to hit a solid and, once we hit it, how much would we have to move along the same direction to get out of it. We’ll call that function \(boxDistance(\vec{ori}, \vec{dir}, \vec{leftBound}, \vec{rightBound})\), where \(\vec{ori}\) is a three-dimensional vector denoting the initial position of the ray, \(\vec{dir}\) is the direction along which we’ll move, and \(\vec{leftBound}\) and \(\vec{rightBound}\) represent the corresponding extreme opposite vertices of the box in space.&lt;/p&gt;

&lt;p&gt;Such a function (optimized to compute cubes) is detailed &lt;a href=&quot;http://jcgt.org/published/0007/03/04/&quot;&gt;in this paper&lt;/a&gt;, and, adapted to our needs, it looks something like the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vec2 boxDistance(vec3 ori, vec3 dir, vec3 leftBound, vec3 rightBound) {
	// dir already inverted
	vec3 t0 = (leftBound - ori) * dir;
	vec3 t1 = (rightBound - ori) * dir;
	vec3 tmin = min(t0, t1);
	vec3 tmax = max(t0, t1);
	float dist_maxmin = max(max(tmin.x, tmin.y), tmin.z);
	float dist_minmax = min(tmax.x, min(tmax.y, tmax.z));
	float dist_to_volume = max(0.0, dist_maxmin);
	float dist_across_volume = max(0.0, dist_minmax - dist_to_volume);
	return vec2(dist_to_volume, dist_across_volume);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The \(\vec{dir}\) vector is inverted \(\frac{1}{\vec{dir}}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The function returns a two component vector. This is done for the sake of convenience and has no geometrical meaning whatsoever; the first component indicates how far away from the volume the origin is, while the second component tells us the distance across the volume.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have the two basic building blocks of our simulated clouds: a density function that samples the different worley noise textures that we provide and a box distance function that allows us to identify whether a ray will intersect a region in space with clouds. Now we just have to make good use of these resources by nicely putting them together in the rendering pipeline.&lt;/p&gt;

&lt;h2 id=&quot;rendering&quot;&gt;rendering&lt;/h2&gt;

&lt;p&gt;Reality itself presents elegant ways to efficiently achieve complex results. Physically based rendering techniques get inspiration from, for example, the way we perceive colors with our eyes; what we can see from reality is just a bunch of photons “bouncing off” objects’ surfaces (although that’s not quite right, it’s okay to simplify it like that). Most PBR techniques try to approximate the famous &lt;a href=&quot;http://www.cse.chalmers.se/edu/year/2011/course/TDA361/2007/rend_eq.pdf&quot;&gt;rendering equation&lt;/a&gt; and &lt;a href=&quot;https://graphics.stanford.edu/courses/cs448-05-winter/papers/nicodemus-brdf-nist.pdf&quot;&gt;bidirectional reflectance distribution function&lt;/a&gt;, both of which are models that accurately compute the behaviour of light. If they’re not approximated, however, they result &lt;em&gt;very&lt;/em&gt; expensive.&lt;/p&gt;

&lt;p&gt;I won’t dwell on the math behind these techniques, since that wouldn’t be faithful to this article’s original purpose (although I strongly encourage any reader interested in this topic to take a look at the papers I linked in the last paragraph). I’ll briefly mention some relevant functions to most cloud-rendering algorithms as well as a few variables specific to my implementation.&lt;/p&gt;

&lt;h3 id=&quot;henyey-greenstein-phase-function&quot;&gt;&lt;em&gt;Henyey Greenstein phase function&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;(As detailed &lt;a href=&quot;https://www.astro.umd.edu/~jph/HG_note.pdf&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;\[ hg(g, \theta) = \frac{1}{4\pi} \frac{1 - g^2}{[1 + g^2 - 2g cos(\theta)]^\frac{3}{2}} \]&lt;/p&gt;

&lt;p&gt;, where \(g\) is a parameter varying in the range [-1, 1] and \(\theta\) is the cosine between the light and ray’s directions.&lt;/p&gt;

&lt;p&gt;The Henyey Greenstein phase function is responsible for producing the silver lining effect, which is due to the light getting diffracted throughout the cloud’s exterior edge, adding a fair amount of realism to the clouds.&lt;/p&gt;

&lt;h3 id=&quot;beers-law&quot;&gt;&lt;em&gt;Beer’s law&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;\[ beer(x) = e^{-x} \]&lt;/p&gt;

&lt;p&gt;, where we may think of \(x\) as the accumulated cloud density along a step through the cloud volume.&lt;/p&gt;

&lt;p&gt;The Lambert-Beer’s function is responsible for the extinction of radiance throughout the volume.&lt;/p&gt;

&lt;h3 id=&quot;a-few-variables&quot;&gt;&lt;em&gt;a few variables&lt;/em&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(samples\) is the number of samples we’ll take along the ray’s path through the volume. It’s used to calculate the step length through the volume.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(inSamples\) is the number of samples we’ll use to compute the in-scattering light arriving at a certain point in the cloud volume from the main light source. It’s used to calculate the step length through the volume.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(radiance\) is the amount of light arriving to a pixel not absorbed by clouds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\vec{color}\) is the resulting cloud’s surface color along the ray.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;in-scattering&quot;&gt;&lt;em&gt;in-scattering&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;The density at a point inside that cloud may seem enough to properly render clouds. However, in reality, a point in a cloud doesn’t only represent the cloud’s density; it also carries the amount of light arriving from a light source to that point. Whenever we sample the density at a point of the cloud, we should also compute how much light is reaching that point. In-scattering accounts for dynamic lighting and shadows in the cloud.&lt;/p&gt;

&lt;p&gt;We’ll accomplish this by raytracing (from the inside of the cloud volume—the point which we are sampling) toward the main light source, calculating the radiance extinction (or cloud density) from the point we were originally sample to the boundaries of the cloud volume and applying Beer’s law.&lt;/p&gt;

&lt;p&gt;If we were to make a function exclusively for this part of the main algorithm, it would look something like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float inScatter(vec3 pos) {
	float distanceInsideVolume = boxDistance(pos, inverseLightDir, cloudLocation - cloudVolume, cloudLocation + cloudVolume).y;
	// &quot;renderShadowingMaxDistance&quot; is a parameter used
	// to specify the maximum distance to compute shadows.
	distanceInsideVolume = min(renderShadowingMaxDistance, distanceInsideVolume);
	float stepSize = distanceInsideVolume / float(inSamples);
	float radiance = 1.0; // all light can reach
	float totalDensity = 0.0;
	for (int i = 0; i &amp;lt; inSamples; ++i) {
		totalDensity += (density(pos) * stepSize);
		pos += lightDir * stepSize;
	}
	return (1.0 - renderShadowingWeight) + beer(total_density) * renderShadowingWeight;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the use of the parameter “renderShadowingWeight”, which regulates how much should the clouds be affected by shadows. If its value is zero, the function will always return a value of 1, which, since it works as a factor in the main rendering algorithm, will make the algorithm ignore in-scattered light.&lt;/p&gt;

&lt;h3 id=&quot;main-algorithm&quot;&gt;&lt;em&gt;main algorithm&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;The idea for our cloud rendering algorithm is to raytrace the cloud volume, integrating the amount of light arriving at each sample position as well as the radiance extinction along the ray’s path.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Initially, the \(radiance\) variable of a pixel equals 1.0 and \(\vec{color}\) is 0.0, meaning light doesn’t get scattered by clouds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We build a ray based on the camera coordinates and the pixel’s normalized direction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using \(boxDistance()\), we check whether the pixel’s ray intersects the cloud volume.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;If the ray &lt;em&gt;does not&lt;/em&gt; intersect the cloud volume, we can safely return 1.0 as the radiance of that pixel, since it won’t intersect any clouds.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;If the ray &lt;em&gt;does&lt;/em&gt; intersect the cloud volume, we’ll compute the Henyey Greenstein value for that particular ray and we’ll proceed to iterate \(samples\) times.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ul&gt;
          &lt;li&gt;We compute the current ray position, the density of the cloud at that position as well as the in-scattering light arriving at that position.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ul&gt;
          &lt;li&gt;We update the radiance value, multiplying it by \(beer(currentDensity * stepSize)\).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ul&gt;
          &lt;li&gt;We update \(\vec{color}\), adding the product of the radiance, the density at the analyzed point, the distance traveled (step length), the in-scattered light arriving to the point, and the Henyey Greenstein constant we computed earlier.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finally, as the definitive pixel color, we can return the color of whatever we rendered previous to the clouds (a skybox perhaps) multiplied by the radiance plus the color vector we calculated for the clouds.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That sure was intense, but the reader shouldn’t be intimidated by the apparent difficulty of the topic; it all makes a lot of sense. In fact, anyone who pondered for long enough at this physical phenomena and was persistant enough could have come up with these approximations.&lt;/p&gt;

&lt;h2 id=&quot;thats-all-folks&quot;&gt;that’s all folks!&lt;/h2&gt;

&lt;p&gt;I hope the reader could get a glance of one possible way to translate clouds to ones and zeros.&lt;/p&gt;

&lt;p&gt;I also hope the reader could appreciate some of the beauty underlying computational physics.&lt;/p&gt;

&lt;p&gt;But, above all, I hope the reader was able to appreciate the power of almighty randomness.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../gifs/ao_final.gif&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;rendered with &lt;a href=&quot;https://github.com/soybin/ao&quot;&gt;ao「青」&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>using computer vision to virtualize motion</title>
	 <link href="https://soybin.github.io/articles/2020/06/20/using-computer-vision-to-virtualize-motion.html"/>
   <updated>2020-06-20T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/articles/2020/06/20/using-computer-vision-to-virtualize-motion</id>
   <content type="html">&lt;h1 id=&quot;using-computer-vision-to-virtualize-motion&quot;&gt;using computer vision to virtualize motion&lt;/h1&gt;

&lt;h2 id=&quot;preamble&quot;&gt;preamble&lt;/h2&gt;
&lt;p&gt;Today, I’ll reflect the logic behind vaaac; a single header file library which uses OpenCV to compute the angles at which a user’s arm is inclined, and to trigger an event right after a very specific motion has been performed by the user’s hand.&lt;br /&gt;
I first wrote the library, and then I wrote &lt;a href=&quot;https://github.com/soybin/cvgo&quot;&gt;&lt;strong&gt;&lt;em&gt;cv:go&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;; an application which overwrites memory values for the player’s view angles in the videogame CS:GO with the values provided by vaaac for the user’s arm inclination. It also allows the user to shoot whenever the motion is performed by their hand, as I demonstrate in the following video:&lt;/p&gt;
&lt;iframe width=&quot;560px&quot; height=&quot;315px&quot; src=&quot;https://www.youtube.com/embed/YiGEf9hP55E?start=1&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;understanding-the-problem&quot;&gt;understanding the problem&lt;/h2&gt;
&lt;p&gt;The problem would be something along these lines:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‘Make a computer understand the real world, given a two-dimensional pixel matrix.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;That problem does certainly look very intimidating. Perhaps because we, as humamns, have more data about our surroundings, such as depth, or perhaps because we don’t fully understand how we, as humans, understand the world around us in the first place.&lt;/p&gt;

&lt;p&gt;Making the problem a bit more specific will make it easier to approach:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‘Make a computer identify a person, compute at what angle their arms are inclined, and check if the user has performed a trigger motion with their hands, by quickly moving their index finger up and back down.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you lack basic image processing knowledge, looking at this problem may seem intimidating. But it becomes easier as you learn about the existence of some methods commonly used in this field.&lt;/p&gt;

&lt;p&gt;The following is how I personally approached the problem, but keep in mind there’s virtually an infinite number of ways of approaching this very same problem.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-users-skin-tone&quot;&gt;getting the user’s skin tone&lt;/h2&gt;
&lt;p&gt;Finding the user’s skin tone is crucial to carrying out the next steps of the process. That’s because we’ll want to differentiate between the user and everything else in the image.&lt;/p&gt;

&lt;p&gt;One smart solution to finding the user’s skin tone can be to use a Haar Cascade in order to find the face of the user, take a sample of the forehead, and apply a threshold. The reson I didn’t end up using this option for vaaac is because Haar cascade files tend to be pretty big, and I wanted to write a library as lightweight as possible.&lt;/p&gt;

&lt;p&gt;What I did instead is quite simple and intuitive, although it requires a bit of work from the user. I asked the user to fill a portion of the screen with a sample of their arm’s skin, and to press any button whenever the requirement was met. The program then takes a copy of the pixel matrix within the screen portion area bounds and computes the average color. As seen in the following gif:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../gifs/vaaac01.gif&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the function used in vaaac to obtain the user’s skin tone:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void calibrateSkinTone() {
	if (ok &amp;amp; 1) {
		int xCoord = res / 2 - SAMPLE_AREA_WIDTH / 2;
		int yCoord = res / 2 - SAMPLE_AREA_HEIGHT / 2;
		int rectSizeX = std::min(SAMPLE_AREA_WIDTH, halfRes);
		int rectSizeY = std::min(SAMPLE_AREA_HEIGHT, halfRes);
		cv::Rect area(xCoord, yCoord, rectSizeX, rectSizeY);
		for (;;) {
			videoCapture &amp;gt;&amp;gt; frame;
			frame = frame(frameBounds);
			if (RENDER_SAMPLE_TEXT) {
				cv::putText(
						frame,
						&quot;fill the area with your skin.&quot;,
						cv::Point(10, area.y - 60),
						cv::FONT_HERSHEY_DUPLEX,
						1.0,
						cv::Scalar(255, 255, 255),
						1);
				cv::putText(
						frame,
						&quot;then press any key.&quot;,
						cv::Point(10, area.y - 20),
						cv::FONT_HERSHEY_DUPLEX,
						1.0,
						cv::Scalar(255, 255, 255),
						1);
			}
			/*
			 * check if user is done before
			 * drawing rectangle because,
			 * otherwise, the rectangle gets
			 * computed as the skin tone mean
			 */
			int key = cv::waitKey(1);
			if (key != -1) {
				break;
			}
			// now draw rectangle and draw
			if (RENDER_TO_WINDOW) {
				cv::rectangle(frame, area, cv::Scalar(255, 255, 255), 2);
				cv::imshow(&quot;calibrateSkinTone&quot;, frame);
			}
		}
		if (RENDER_TO_WINDOW) {
			cv::destroyWindow(&quot;calibrateSkinTone&quot;);
		}
		cv::Mat hsv;
		cv::cvtColor(frame, hsv, cv::COLOR_BGR2HSV);
		cv::Mat sample(hsv, area);
		cv::Scalar mean = cv::mean(sample);
		hLow = mean[0] - MASK_LOW_TOLERANCE;
		hHigh = mean[0] + MASK_HIGH_TOLERANCE;
		sLow = mean[1] - MASK_LOW_TOLERANCE;
		sHigh = mean[1] + MASK_HIGH_TOLERANCE;
		vLow = 0;
		vHigh = 255;
		ok = 2;
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;binarization-of-the-image&quot;&gt;binarization of the image&lt;/h2&gt;
&lt;p&gt;After finding the user’s color skin, the next step is to binarize the image. That means making every pixel of the image either black (if it’s not the user’s skin) or white (if it’s the user’s skin).&lt;br /&gt;
First of all we’ll convert the RGB image to HSV, that way we can ignore shadows.&lt;br /&gt;
Then we’ll check for every pixel in the image whether that pixel is equal to the average color of the user’s skin or not. However, this won’t work because of how lighting affects the color of the user’s skin. Instead, we have to check for every pixel in the image whether that pixel can be found within a range of values. This can be easily done with the OpenCV function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv::inRange(cv::Mat, cv::Scalar(skinTone - lowThreshold), cv::Scalar(skintTone + highThreshold), cv::Mat)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;isolate-a-component-with-breadth-first-search&quot;&gt;isolate a component with breadth first search&lt;/h2&gt;
&lt;p&gt;Breadth first search (bfs from now on) is a basic graph traversing algorithm (take a look &lt;a href=&quot;https://en.wikipedia.org/wiki/Breadth-first_search&quot;&gt;&lt;strong&gt;&lt;em&gt;here&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; in case you don’t know much about it).&lt;br /&gt;
A matrix such as the recently computed binary image can be easily interpreted as a graph. Now we can make an assumption, or rather a requirement: the elbow of the user must be fixed nearby the center of the image. This is completely natural, as the user would adopt this position anyways to get more screen space to aim.&lt;br /&gt;
By creating this restriction, we can have a small area in the center of the image that serves as the trigger to identify the bounds of the user’s arm.
This means that we can check every pixel of that small area for every frame, and apply bfs only if skin has been found.&lt;br /&gt;
By applying bfs we can get the bounds of the area occupied by the arm, as well as the furthest point from the center of the screen, in other words, the position of the tip of the index finger of the user’s arm.&lt;/p&gt;

&lt;p&gt;It’s worth noting that applying bfs for every single pixel would be extremely inefficient, so I added a variable &lt;em&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;/em&gt; that represents the square root of the area in pixels of each new subdivision of the image.&lt;br /&gt;
That just means that I don’t apply bfs to every individual pixel, rather I compute the average value for every subdivision of the screen with area \(2^n\).&lt;/p&gt;

&lt;p&gt;The bfs algorithm works the following way:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For every subdivision inside the area at the center of the image, check the average color of the subdivision.
    &lt;ul&gt;
      &lt;li&gt;If a subdivision has non-zero value, stop checking every subdivision inside the area at the center of the image, and start bfs by adding this subdivision to the bfs queue for processing.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;While the bfs queue is not empty
    &lt;ul&gt;
      &lt;li&gt;Update \((x, y)\) coordinate values to the current subdivision position coordinates (relative to the center of the screen). Since bfs will check the farthest point in the last place, no check is necessary.&lt;/li&gt;
      &lt;li&gt;Check every neighbor subdivision of the currently processed subdivision, and enqueue the neighbors with non-zero value.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a simplification, but the actual implementation in vaaac is not that different.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (; !q.empty(); ) {
	std::pair&amp;lt;int, int&amp;gt; xy = q.front();
	int x = xy.first, y = xy.second;
	q.pop();
	if (x &amp;lt; 0 || y &amp;lt; 0 || x + BFS_SAMPLE_SIZE &amp;gt; res || y + BFS_SAMPLE_SIZE &amp;gt; res || visited[x][y]) continue;
	visited[x][y] = true;
	if (cv::mean(mask(cv::Rect(x, y, BFS_SAMPLE_SIZE, BFS_SAMPLE_SIZE)))[0] == 0) continue;
	/*
	 * update aiming point coordinates.
	 * works because the bfs algorithm always
	 * visits the farthermost element in the
	 * last place
	 */
	xAim = x;
	yAim = y;
	// update found object area bounds
	xMin = std::min(xMin, x);
	yMin = std::min(yMin, y);
	xMax = std::max(xMax, x);
	yMax = std::max(yMax, y);
	// add neighbors to queue
	for (auto&amp;amp; offset : bfsOffsets) {
		q.push(std::make_pair&amp;lt;int, int&amp;gt;(x + offset.first, y + offset.second));
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that’s left to do is compute the aiming angles in degrees relative to the image bounds, and smooth them by an arbitrary factor.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// make angles
yAngle = -(double)(halfRes - yAim) / halfRes * 90.0;
xAngle = -(double)(halfRes - xAim) / halfRes * 90.0;
yAngleSmooth += (yAngle - yAngleSmooth) / (double)(AIM_SMOOTHNESS);
xAngleSmooth += (xAngle - xAngleSmooth) / (double)(AIM_SMOOTHNESS);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;event-trigger&quot;&gt;event trigger&lt;/h2&gt;
&lt;p&gt;I wanted to have some way for the user to communicate an action to the library, because just computing the arm inclination angles is probably not enough for most usage cases.&lt;br /&gt;
The motion of moving your index finger up and back down would be pretty convenient, since the main idea I had in mind for this library was to integrate it with an fps videogame, so that’s what I did.&lt;/p&gt;

&lt;p&gt;This problem is fairly easy, but it needed some more work than I initially expected.&lt;br /&gt;
The data structure I used to solve it is a double ended queue, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::deque&amp;lt;T&amp;gt;&lt;/code&gt; in c++ stl. This is because we’ll want to remove elements from the back and add to the front, but still be able to access the elements in between.&lt;br /&gt;
Anyways, it boils down to the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the deque is empty. If that’s the case, add the current index finger coordinates to it and continue.&lt;/li&gt;
  &lt;li&gt;Else if the bool &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; is true. If that’s the case, check if the y-coordinate of the index finger is greater than that of the last deque element. Otherwise, check if the distance between the last enqueued y-coordinate and the first enqueued y-coordinate is greater or equal than a certain threshold (this is to avoid noise from being detected as an event). If that’s the case, set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; to false. Otherwise, clear the deque.&lt;/li&gt;
  &lt;li&gt;Else if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; is set false, check if the y-coordinate of the index finger is smaller than that of the last deque element. Otherwise, check if the distance between the last enqueued y-coordinate and the first enqueued y-coordinate is greater or equal than a certain threshold (this is to avoid noise from being detected as an event). If that’s the case, set the boolean &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;triggered&lt;/code&gt; to true meaning that, in this frame, an event was triggered by the user. Then clear the deque and set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;increment&lt;/code&gt; to true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is the code used in vaaac to implement the previous concept. Remember that you can take a full look to the source code at &lt;a href=&quot;https://github.com/soybin/vaaac&quot;&gt;&lt;em&gt;&lt;strong&gt;https://github.com/soybin/vaaac&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if (!yxDeltaSize) {
	yxDelta.push_back({ yAim, xAim });
	yxDeltaSize = 1;
} else if (increment) {
	if (yAim &amp;lt;= yxDelta[yxDeltaSize - 1].first) {
		++yxDeltaSize;
		yxDelta.push_back({ yAim, xAim });
	} else {
		int yDelta = yxDelta[0].first - yxDelta[yxDeltaSize - 1].first;
		if (yDelta &amp;gt;= TRIGGER_MINIMUM_DISTANCE_PIXELS &amp;amp;&amp;amp; yDelta &amp;lt;= TRIGGER_MAXIMUM_DISTANCE_PIXELS) {
			increment = false;
		} else {
			yxDeltaSize = 0;
			yxDelta.clear();
		}
	}
} else {
	if (yAim &amp;gt;= yxDelta[yxDeltaSize - 1].first) {
		yxDelta.push_back({ yAim, xAim });
		++yxDeltaSize;
	} else {
		std::pair&amp;lt;int, int&amp;gt; left = yxDelta[0];
		std::pair&amp;lt;int, int&amp;gt; right = yxDelta[yxDeltaSize - 1];
		bool ok = true;
		ok &amp;amp;= std::abs(left.first - right.first) &amp;lt;= TRIGGER_ALLOWED_Y_DEVIATION_PIXELS;
		ok &amp;amp;= std::abs(left.second - right.second) &amp;lt;= TRIGGER_ALLOWED_X_DEVIATION_PIXELS;
		if (ok) {
			triggered = true;
			yAim = left.first;
			xAim = left.second;
		}
		increment = true;
		yxDelta.clear();
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;This was a very fun project to work on. I had never done anything related to image processing, but the learning curve was mostly linear thanks to OpenCV’s design and comprehensive documentation.&lt;br /&gt;
Computer vision has an exciting long way to go. The possibilities are very exciting and, in the literal sense of the word, endless.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>raymarch the geometry & pathtrace the scene</title>
	 <link href="https://soybin.github.io/articles/2020/05/22/raymarch-the-geometry-and-pathtrace-the-scene.html"/>
   <updated>2020-05-22T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/articles/2020/05/22/raymarch-the-geometry-and-pathtrace-the-scene</id>
   <content type="html">&lt;!---[*fractal rendered with idyll using raymarching and path tracing*](/images/idyll/first.png){:width=&quot;100%&quot;}---&gt;
&lt;h1 id=&quot;raymarch-the-geometry--pathtrace-the-scene&quot;&gt;raymarch the geometry &amp;amp; pathtrace the scene&lt;/h1&gt;

&lt;h2 id=&quot;preamble&quot;&gt;preamble&lt;/h2&gt;
&lt;p&gt;Today, I’ll superficially explain and reflect on raymarching and pathtracing; the two main algorithms behind the rendering pipeline that I came up with when developing  &lt;a href=&quot;https://www.youtube.com/watch?v=cFykbtJmg4A&quot;&gt;&lt;strong&gt;&lt;em&gt;idyll&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;, a fractal engine.&lt;/p&gt;

&lt;h2 id=&quot;definitions&quot;&gt;definitions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ray&lt;/em&gt;: simple structure composed of two \(n\) dimensional vectors: an &lt;strong&gt;origin&lt;/strong&gt; vector, which indicates at which point in space the ray originates, and a unit &lt;strong&gt;direction&lt;/strong&gt; vector, which denotes the direction in which the ray will move.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Geometry&lt;/em&gt;: any independent model, shape or object present in the scene.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Scene&lt;/em&gt;: \(n\) dimensional space in which the geometry is represented.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;raymarching&quot;&gt;raymarching&lt;/h2&gt;
&lt;p&gt;Raymarching is a raycasting-based algorithm used to interpret a scene.&lt;/p&gt;

&lt;p&gt;The algorithm itself doesn’t compute lighting, shadowing nor reflections on its own; it merely provides information about whether a ray emitted from a pixel does or doesn’t intersect the surface of the geometry in the scene. In other words, it computes how far away the geometry is from a view matrix. However, with this information and some slight modifications, one can calculate all sorts of interesting stuff, such as ambient occlusion, the normal vector of a point in the geometry’s surface, or shadowing.&lt;/p&gt;

&lt;h3 id=&quot;distance-estimators&quot;&gt;distance estimators&lt;/h3&gt;
&lt;p&gt;Assume that we’re working in 3-dimensional space; &lt;em&gt;in order to perform raymarching, we need a function that tells us how far away is any given point in space from the closest point to the surface of the geometry&lt;/em&gt;. That’s called a &lt;strong&gt;&lt;em&gt;distance estimator&lt;/em&gt;&lt;/strong&gt;—DE from now on.&lt;/p&gt;

&lt;p&gt;The most basic DE functions represent primitive objects. &lt;a href=&quot;https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm&quot;&gt;&lt;strong&gt;&lt;em&gt;This&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; is a fantastic article by Íñigo Quílez on them.&lt;/p&gt;

&lt;p&gt;Here’s the DE function for a sphere \(sphereDE\), where \(P\) is the point in space we’re calculating how far apart from the sphere is, and \(S\) is the scale of the sphere (assuming the sphere is located at the origin of coordinates):&lt;/p&gt;

&lt;p&gt;\[ sphereDE(P, S) = length(P) - S\]&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;double sphereDE(vector3 P, double S) {
	return length(P) - S;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;what-to-do-with-a-de&quot;&gt;what to do with a DE&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Once you know how far away from something you are, you know you can safely move that distance in any direction, without having to worry about prematurely intersecting the geometry.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let a camera be placed anywhere in the scene with a distance of 10 meters from the origin of coordinates (0, 0, 0) and be looking toward it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let a ray have its origin (ori) at the position of the camera in the scene, and give it a direction as a normalized vector (dir), which should be obtained from the view matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let the variable (tDist) represent how far away has the ray moved from its origin. Initialize it to zero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, calculate the DE from the point ori (the origin of the ray).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is safe to &lt;em&gt;&lt;strong&gt;march&lt;/strong&gt;&lt;/em&gt; the ray by the distance returned by the DE function; add DE(ori) to tDist, and use the expression DE(ori + dir * tDist) from now on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat this process by computing the DE for ori + dir * tDist until it returns a value smaller than a certain arbitrary threshold (1e-4, for instance), meaning that the ray is close enough to the surface of the geometry for it to be considered an intersection. Obviously, the ray won’t get to hit the geometry itself because of Zeno’s paradox; it’s an arbitrarily accurate estimation, hence the name distance estimator.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This process should be computed for each pixel on the screen, and it’ll provide an accurate interpretation of the geometry, no matter how complex it may be.&lt;/p&gt;

&lt;p&gt;The pseudocode for the simplest version of this algorithm would look something like the following:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// distance threshold
const double MIN_DISTANCE = 1e-4

/*
 * the maximum number of steps allowed. 
 * if this amount of steps, 'raymarch'
 * will return 0.0
 */
const int MAX_STEP = 64
 
color raymarch(vector3 ori, vector3 dir) {
	double tDist = 0.0
	int i
	for (i = 0; i &amp;lt; MAX_STEP; ++i) {
		double curDist = sphereDE(ori + dir * tDist)
		// in case the threshold is reached
		if (curDist &amp;lt; MIN_DISTANCE) {
			break
		}
		// else, add the marched distance
		// to the distance variable
		tDist += curDist
	}
	return color(1.0 - (double)i / (double)MAX_STEP)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a very simple implementation, and if we were to use the returned value from this function as the value for the three color channels of each pixel, we would get a black and white image with some very obvious banding artifacts (since we’re deriving the return value from two integers).&lt;/p&gt;

&lt;p&gt;There’s a lot of things regarding DEs I’ve left out of this explanation because they’re beyond the scope of this article, such as primitive shapes transformation and combination (somewhat similar to intersection boolean operations), spatial transforms, and, my favorite, &lt;a href=&quot;https://www.youtube.com/watch?v=zy1l1SLJDo4&quot;&gt;modular repetition&lt;/a&gt;, among many others. Slightly modifying the DE for the geometry will dramatically alter its end result.&lt;/p&gt;

&lt;h3 id=&quot;ray-direction&quot;&gt;ray direction&lt;/h3&gt;
&lt;p&gt;The following is not vital to understand raymarching; however, it is a basic implementation detail. To calculate a pixel’s ray direction given a screen resolution \(width\) and \(height\) and a desired field of view \(fov\), I use the following expression:&lt;/p&gt;

&lt;p&gt;Let the constant \(P_x\) be equal to the pixel x-coordinate on the screen.&lt;br /&gt;
Let the constant \(P_y\) be equal to the pixel y-coordinate on the screen.&lt;br /&gt;
Let the constant \(width\) be equal to the screen’s width in pixels.&lt;br /&gt;
Let the constant \(height\) be equal to the screen’s height in pixels.&lt;/p&gt;

&lt;p&gt;\[rayDirection(P_x, P_y) = [x, y, -z]\]
\[x = P_x + 0.5 - \frac{width}{2}\]
\[y = P_y + 0.5 - \frac{height}{2}\]
\[z = \frac{height}{tan(\frac{fov}{2})}\]&lt;/p&gt;

&lt;p&gt;This expression comes in handy when using a fragment shader on a quad to render a scene.&lt;/p&gt;

&lt;h3 id=&quot;usage&quot;&gt;usage&lt;/h3&gt;
&lt;p&gt;As long as one can derive a function that, for any given point in 3d space, returns its distance from the closest point to any geometry in the scene, one can render pretty much anything with a great performance. Adding dynamic lighting, shadowing, occlusion and surface color would have a very low computational cost. Even changing the geometry’s scale, position and rotation. Anything is valid, because everything is dynamic and computed in real time.&lt;/p&gt;

&lt;p&gt;Raymarching is not mainstream in the videogame industry, and the reason for this is that &lt;strong&gt;&lt;em&gt;a DE function is required&lt;/em&gt;&lt;/strong&gt;; and in order for it to run in real time, the whole scene should to be computed in a fragment shader on the GPU side. When dealing with a world with hundreds of independent entities interacting with each other, materials to be computed, and very specific and complex geometrical shapes, it becomes counterproductive to have an efficient DE at the core of the rendering pipeline.&lt;/p&gt;

&lt;p&gt;On the other hand, whenever the geometry can be expressed through a distance estimator, raymarching is, by far, the most elegant and efficient technique. This makes raymarching the ideal method to render complex mathematical models, such as fractals.&lt;/p&gt;

&lt;h2 id=&quot;pathtracing&quot;&gt;pathtracing&lt;/h2&gt;
&lt;p&gt;It could be said that pathtracing derives from raytracing. However, the differences between these terms can be confusing and we’ll deal with them later in this article. As for now, I’ll briefly explain what pathtracing is all about.&lt;/p&gt;

&lt;p&gt;There are two main constants to define when implementing a pathtracing algorithm: samples per pixel and bounces per ray.&lt;/p&gt;

&lt;p&gt;The idea behind pathtracing is to &lt;em&gt;&lt;strong&gt;iteratively integrate most of the light arriving to a point in the surface of the geometry&lt;/strong&gt;&lt;/em&gt;, so that the image looks as real as possible.
To visualize it, this is the pseudocode for a very basic pathtracing algorithm (only Lambertian reflection):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// number of samples per pixel
const int SAMPLES = 256

// maximum number of allowed ray bounces
const double BOUNCES = 8;

// probability of ray reflection
const double PROB = 1/PI

// pathtracing recursively
color pathtrace (ray r, int bounces) {
	// in the ray missed
	if (!r.hit) return color(0.0)

	// in case we reached max number of bounces
	if (bounces == BOUNCES) return color(0.0)

	// get ray normal
	vector3 rayNormal = calculateNormal(r.origin, r.direction)

	// get current new ray origin
	vector3 newOrigin = r.origin + r.length * r.direction;

	// get random unit vector in the hemisphere
	// of the normal, so that the ray can bounce off
	vector3 newDir = getHemisphereRandom(rayNormal)

	// get color at the intersected point
	color color = getWorldColor(r.origin)

	// bidirectional reflectance distribution function
	double ct = dot(newDir, rayNormal)
	color brdf = r.hitReflectance / PI

	ray newRay = {newOrigin, newDir}

	color nextBounce = pathtrace(newRay, bounces + 1)

	// this is where the integration of
	// the lighting along the different
	// intersection points of the ray
	// happens
	return color + (brdf * nextBounce * ct / PROB)
}

// iterate through every pixel in the screen
for (y in height) {
	for (x in width) {
		// get ray's direction and origin
		vector3 direction = rayDirection(x, y)
		vector3 origin = {0, 0, 10.0}
		// variable storing color
		color totalColor = {0.0, 0.0, 0.0}
		// iterate for every sample and add to color
		for (sample in SAMPLES) {
			totalColor += pathtrace(origin, direction, 0)
		}
	}
	color /= samples
	pixels[y][x] = color
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What’s happening here is the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Iterate through every pixel in the screen or image.&lt;/li&gt;
  &lt;li&gt;~ Compute the ray’s origin and direction from the pixel coordinates.&lt;/li&gt;
  &lt;li&gt;~ Iterate through every sample.&lt;/li&gt;
  &lt;li&gt;~ ~ Add the return value of the function ‘pathtrace’, which randomly computes a possible value for that ray, to the ‘totalColor’ variable.&lt;/li&gt;
  &lt;li&gt;~ Divide the ‘totalColor’ variable by the number of samples to get the average value for all of the samples.&lt;/li&gt;
  &lt;li&gt;~ Set the pixel with ‘x’ and ‘y’ coordinates to the ‘totalColor’ variable value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With a high enough value for the samples and bounces variable, one can get very photorealistic images using path tracing.&lt;/p&gt;
&lt;h2 id=&quot;disambiguation&quot;&gt;disambiguation&lt;/h2&gt;

&lt;p&gt;If you’re reading this, chances are you’ve heard of raytracing, and it’s no coincidence raytracing, raymarching, and pathtracing all sound confusingly familiar.&lt;/p&gt;

&lt;p&gt;All three of these PBR (physically based rendering) algorithms have something in common: &lt;em&gt;&lt;strong&gt;they all use raycasting to understand a scene’s geometry&lt;/strong&gt;&lt;/em&gt;, but what each one of this algorithms accomplishes, and the way they operate is different from one another. It’s also true that most researchers have slightly different understandings of these concepts, since the lines separating them are often blurry.
I’ll try to shed some light regarding the differences between these techniques based on my own experience and understanding.&lt;/p&gt;

&lt;h3 id=&quot;raymarching-and-raytracing&quot;&gt;raymarching and raytracing&lt;/h3&gt;
&lt;p&gt;In raymarching, the intersection with the scene’s geometry is computed using a distance estimator, as we’ve recently seen. On the other hand, raytracing computes the intersection analitically, not requiring a DE. Coming up with a DE for a fractal or any mathematical model can be easy, but when there are many objects with different shapes, raymarching grows exponentially in price. This makes raytracing―although still pretty expensive―more suitable for games, and it’s also the reason why raytracing isn’t, and probably won’t be, the mainstream rendering technique.&lt;br /&gt;
That being said, raymarching is awesome; as I mentioned earlier, when used properly, it can deliver unprecedented performance on very specific applications, and its simplicity makes it very straightforward to understand, implement, and maintain.&lt;/p&gt;

&lt;h3 id=&quot;pathtracing-and-raytracing&quot;&gt;pathtracing and raytracing&lt;/h3&gt;
&lt;p&gt;Pathtracing is a physically based rendering (PBR) technique that relies on the monte carlo method. It integrates the amount of light arriving to a specific point in a surface of an object in the scene by averaging out different paths that the ray could take when bouncing off that surface. That’s the overall idea, but whether that point be the camera lenses or an object’s surface depends on the specific implementation.&lt;br /&gt;
In the case of &lt;strong&gt;&lt;em&gt;idyll&lt;/em&gt;&lt;/strong&gt;, each point of intersection from a camera ray intersecting the geometry is pathtraced to compute the amount of light arriving to that point; a combination of pathtracing and raytring.&lt;/p&gt;

&lt;h2 id=&quot;union-of-raymarching-and-pathtracing&quot;&gt;union of raymarching and pathtracing&lt;/h2&gt;
&lt;p&gt;Raymarching and pathtracing can be complementary to each other, making for a solid rendering pipeline:&lt;br /&gt;
With raymarching, one can check whether a ray intersects the geometry and where. Then, one can use pathtracing to integrate the global illumination arriving at the point of intersection between the surface of the geometry and the marched ray.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;&lt;strong&gt;idyll&lt;/strong&gt;&lt;/em&gt;, pathtracing computes the global lighting, but doesn’t compute the point of intersection with the geometry. On the other hand, raymarching computes the point of intersection with the geometry, but doesn’t compute lighting. Although this is an oversimplification, since the actual relation between these two techniques is a bit more entangled.&lt;/p&gt;
&lt;h2 id=&quot;results&quot;&gt;results&lt;/h2&gt;
&lt;p&gt;After properly implementing these techniques, and throwing some iterative fractals into the mix, I developed &lt;a href=&quot;https://github.com/soybin/idyll&quot;&gt;&lt;strong&gt;&lt;em&gt;idyll&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;, a fractal engine that can create beautiful renders like the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/idyll/first.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/images/idyll/second.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/images/idyll/third.png&quot; alt=&quot;*fractal rendered with idyll using raymarching and path tracing*&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 
 
 <entry>
   <title>probability, stoicism, and happiness</title>
	 <link href="https://soybin.github.io/thoughts/2020/08/13/probability-stoicism-and-happiness.html"/>
   <updated>2020-08-13T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/thoughts/2020/08/13/probability-stoicism-and-happiness</id>
   <content type="html">&lt;h1 id=&quot;probability-stoicism-and-happiness&quot;&gt;probability, stoicism, and happiness&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What are the chances of you tossing a coin and getting heads?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Well, 50% is the obvious answer.&lt;/p&gt;

&lt;p&gt;Different coins from different countries have different physical attributes; some of them differ in weight distribution, shape, size and overall aerodynamic capabilities. Those slight variations may affect the likelihood of you getting heads.&lt;/p&gt;

&lt;p&gt;To get to the point in your life where you have to toss a coin, a lot of things must’ve happened. If we start taking in count the odds for every event in your life that lead you to that exact moment, the odds of you throwing a coin and getting heads一or cross一get very close to absolute zero.&lt;/p&gt;

&lt;p&gt;The chances for the kinds of events discussed so far happening can be more or less estimated. But there are events for which we can’t, as humans, possibly estimate the chances of. For example, what are the odds of &lt;em&gt;you&lt;/em&gt; being who &lt;em&gt;you&lt;/em&gt; are? or, what are the odds of the universe we’re in right now being created in the first place? The idea of calculating the probability for these events is simply ridiculous. Perhaps part of the reason is that we don’t deeply understand what these events really imply, or perhaps not even Gods know why they’re alive. If that were to be the case, what makes you any different from a God?&lt;/p&gt;

&lt;p&gt;I don’t like feeling like everything is an absolute mess and nonsense chaos so, at times, I like to think about the universe as a game. The only thing a game needs in order to exist are a set of rules, so here are my rules to the universe一at least the ones that apply to me; a player:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Everything is randomly decided and has no reason to be.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I mean, it could be worse.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>future self,</title>
	 <link href="https://soybin.github.io/thoughts/2020/07/20/future-self,.html"/>
   <updated>2020-07-20T00:00:00+02:00</updated>
	 <id>https://soybin.github.io/thoughts/2020/07/20/future-self,</id>
   <content type="html">&lt;h1 id=&quot;future-self&quot;&gt;future self,&lt;/h1&gt;

&lt;p&gt;don’t lose sight of your resolve&lt;/p&gt;
</content>
 </entry>
 
</feed>
